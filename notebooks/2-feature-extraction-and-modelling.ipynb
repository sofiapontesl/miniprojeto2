{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Features e Construção do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introdução a Extração de Features\n",
    "- A extração de features é uma parte muito importante na análise e na identificação de relações entre diferentes elementos. Como já sabemos, os dados de áudio não podem ser compreendidos diretamente pelos modelos, então precisamos convertê-los para um formato inteligível, e é para isso que a extração de features é utilizada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com a taxa de amostragem e os dados do sinal, podemos realizar diversas transformações para extrair características valiosas do áudio. No entanto, neste projeto, não vamos aprofundar no processo de seleção de features para identificar quais são mais relevantes para o nosso dataset. Em vez disso, vamos extrair cinco features principais para treinar nosso modelo:\n",
    "- **Zero Crossing Rate (ZCR):** Mede a taxa de mudanças de sinal no áudio, ou seja, quantas vezes ele cruza o eixo zero em um determinado intervalo de tempo. Essa feature é útil para distinguir sons percussivos e não percussivos.\n",
    "- **Chroma STFT:** Representa a energia espectral em 12 bins correspondentes às notas da escala musical ocidental. Essa característica é útil para identificar padrões harmônicos no áudio.\n",
    "- **MFCC (Mel-Frequency Cepstral Coefficients):** Converte a frequência do áudio para a escala mel, aproximando-se da percepção auditiva humana. É uma das features mais utilizadas em reconhecimento de fala e emoção.\n",
    "- **RMS (Root Mean Square):** Mede a energia do sinal ao calcular a média quadrática das amplitudes do áudio. Essa feature ajuda a representar a intensidade do som.\n",
    "- **Mel Spectrogram:** Representa a distribuição de energia do áudio em diferentes faixas de frequência na escala mel, capturando informações espectrais essenciais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Carregar o Dataframe da EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Trabalho\\work_soso\\audio_trilha\\miniprojeto2\\data\\ravdess_preprocessed.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "      <th>Processed_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_01/03-01-01...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_01/03-01-01...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_01/03-01-01...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_01/03-01-01...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_01/03-01-02...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>surprise</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_24/03-01-08...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>surprise</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_24/03-01-08...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>surprise</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_24/03-01-08...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>surprise</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_24/03-01-08...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>surprise</td>\n",
       "      <td>../Audio_Speech_Actors_01-24/Actor_24/03-01-08...</td>\n",
       "      <td>c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotions                                               Path  \\\n",
       "0      neutral  ../Audio_Speech_Actors_01-24/Actor_01/03-01-01...   \n",
       "1      neutral  ../Audio_Speech_Actors_01-24/Actor_01/03-01-01...   \n",
       "2      neutral  ../Audio_Speech_Actors_01-24/Actor_01/03-01-01...   \n",
       "3      neutral  ../Audio_Speech_Actors_01-24/Actor_01/03-01-01...   \n",
       "4         calm  ../Audio_Speech_Actors_01-24/Actor_01/03-01-02...   \n",
       "...        ...                                                ...   \n",
       "1435  surprise  ../Audio_Speech_Actors_01-24/Actor_24/03-01-08...   \n",
       "1436  surprise  ../Audio_Speech_Actors_01-24/Actor_24/03-01-08...   \n",
       "1437  surprise  ../Audio_Speech_Actors_01-24/Actor_24/03-01-08...   \n",
       "1438  surprise  ../Audio_Speech_Actors_01-24/Actor_24/03-01-08...   \n",
       "1439  surprise  ../Audio_Speech_Actors_01-24/Actor_24/03-01-08...   \n",
       "\n",
       "                                         Processed_Path  \n",
       "0     c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "1     c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "2     c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "3     c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "4     c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "...                                                 ...  \n",
       "1435  c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "1436  c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "1437  c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "1438  c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "1439  c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Tra...  \n",
       "\n",
       "[1440 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Criando uma função para extração das features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fizemos essa parte no notebook passado, então você pode copiar e colar o código das funções aqui, pois precisaremos delas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    n = np.random.normal(0, data.std(), data.size) # cria um vetor com ruído branco \n",
    "    return data + n # retorna o áudio com ruido\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate=rate) # retonra um áudio com velocidade alterada\n",
    "\n",
    "def shift(data):\n",
    "    return np.roll(data, np.random.random(1, len(data))) # retorna um vetor de audio com os dados deslocados aleatoriamente\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor) # retorna um audio com a frequência alterada\n",
    "\n",
    "# Tomando um áudio qualquer com exemplo\n",
    "path = np.array(df.Path)[1]\n",
    "data, sample_rate = librosa.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aqui você deve extrair essas features de fato... mais tarde você precisará delas :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, sample_rate): \n",
    "    result = np.array([])\n",
    "    # zcr\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data, pad=False), axis=1)\n",
    "    result = np.hstack((result, zcr))\n",
    "\n",
    "    # Chroma_stft\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=data, sr=sample_rate), axis=1)\n",
    "    result = np.hstack((result, chroma_stft)) \n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate), axis=1) # n_mfcc = 20, pois com 20 amostras já é possível detectar padrões no áudio \n",
    "    result = np.hstack((result, mfcc))\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data, frame_length=2048, hop_length=512), axis=1)\n",
    "    result = np.hstack((result, rms))\n",
    "\n",
    "    # MelSpectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr = sample_rate), axis=1)\n",
    "    result = np.hstack((result, mel))\n",
    "    \n",
    "    return result # retorna um array com 1x162, em que cada coluna é o dado de uma feaure \n",
    "\n",
    "def get_features(path):\n",
    "    # Carregar áudio\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "\n",
    "    # Sem aumento de dados\n",
    "    res1 = extract_features(data, sample_rate)  \n",
    "    result = np.array(res1)\n",
    "\n",
    "    # Com ruído\n",
    "    noise_data = noise(data) # aplica áudio com ruído\n",
    "    res2 = extract_features(noise_data, sample_rate)  \n",
    "    result = np.vstack((result, res2))\n",
    "\n",
    "    # Com alongamento e mudança de pitch\n",
    "    new_data = stretch(data) # altera o tempo\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate) # altera a afinação (pitch) \n",
    "    res3 = extract_features(data_stretch_pitch, sample_rate) \n",
    "    result = np.vstack((result, res3)) # result é o empilhamento de linhas de todos as features\n",
    "    \n",
    "    return result # retorna um array 3x162, em que todas as linhas representam o mesmo áudio e as linhas são as features desse áudio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rode as células abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for path, emotion in zip(df['Path'], df['Emotions']):\n",
    "    feature = get_features(path) \n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        Y.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4320, 4320, (1440,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(Y), df.Path.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agora você deverá salvar o csv de features (features.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo foi salvo em: c:\\Users\\sofip\\OneDrive\\Documentos\\Área de Trabalho\\work_soso\\audio_trilha\\miniprojeto2\\data\\features.csv\n"
     ]
    }
   ],
   "source": [
    "# Definir o caminho correto para a pasta 'data' na raiz do projeto\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Caminho correto para salvar o CSV na pasta 'data' da raiz do projeto\n",
    "# Crie um DataFrame do pandas chamado Features usando a lista X.\n",
    "# Adicione uma coluna 'labels' ao DataFrame usando a lista Y.\n",
    "# Salve o DataFrame como um arquivo CSV no caminho definido, sem incluir o índice, definindo _index_=False\n",
    "# Code here\n",
    "csv_path = os.path.join(data_dir, \"features.csv\")\n",
    "#csv_path = os.path.join(\"planilha.csv\")\n",
    "Features = pd.DataFrame(X)\n",
    "Features['labels'] = Y\n",
    "Features.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"O arquivo foi salvo em: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Preparation (Preparação dos Dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Features.iloc[: ,:-1].values\n",
    "Y = Features['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder para transformar o Y em uma representação binária categórica, necessária para problemas de classificação multiclasse\n",
    "# Code heretry:\n",
    "Y = np.array(Y).reshape(-1,1)\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use train_test_split do scikit-learn para dividir X e Y em conjuntos de treino e teste\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Code here\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m x_train, x_test , y_train, y_test = train_test_split(X,\u001b[43my_encoded\u001b[49m, test_size=\u001b[32m0.25\u001b[39m,random_state=\u001b[32m42\u001b[39m )\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain = x =\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m y =\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, test= x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m y=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "# Use train_test_split do scikit-learn para dividir X e Y em conjuntos de treino e teste\n",
    "# Code here\n",
    "\n",
    "x_train, x_test , y_train, y_test = train_test_split(X,y_encoded, test_size=0.25,random_state=42 )\n",
    "print(f'train = x ={x_train.shape} y ={y_train.shape}, test= x={x_test.shape} y={y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize StandardScaler do sklearn para normalizar as características de X\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rode essa célula para deixar as dimensões certinhas com o modelo que iremos criar.\n",
    "x_train = np.expand_dims(x_train, axis=2)\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training (Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O modelo que vamos usar é uma rede neural convolucional (CNN) projetada para processar os dados extraídos dos arquivos de áudio. Essa estrutura é ideal para capturar padrões espectrais, como variações de tom e intensidade. A CNN é composta por:\n",
    "- Camadas Convolucionais (Conv1D): Extraem características do espectro do áudio.\n",
    "- Camadas de Pooling (MaxPooling1D): Reduzem a dimensionalidade e capturam as informações mais relevantes.\n",
    "- Dropout: Ajuda a evitar overfitting.\n",
    "- Camada Flatten: Transforma os mapas de features em um vetor de entrada para a camada totalmente conectada.\n",
    "- Camadas Densas (Dense): Realizam a classificação final usando a função de ativação softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dica: você pode olhar a documentação e ir seguindo o passo a passo arquitetônico para criar o modelo.\n",
    "\n",
    "# Passo 1: Use Sequential() para criar o modelo como um contêiner linear.\n",
    "# Passo 2: Adicione uma camada Conv1D com 256 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 3: Siga com uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 4: Adicione mais uma camada Conv1D com 256 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 5: Adicione mais uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 6: Adicione mais uma camada Conv1D com 128 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 7: Adicione mais uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 8: Adicione uma camada Dropout com 0.2 de taxa de dropout.\n",
    "# Passo 9: Adicione mais uma camada Conv1D com 64 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 10: Adicione mais uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 11: Adicione uma camada Flatten.\n",
    "# Passo 12: Adicione uma camada Dense com 32 unidades e função de ativação 'relu'.\n",
    "# Passo 13: Adicione uma camada Dropout com 0.3 de taxa de dropout.\n",
    "# Passo 14: Adicione uma camada Dense com 8 unidades e função de ativação 'softmax'.\n",
    "# Passo 15: Compile o modelo com otimizador 'adam', loss 'categorical_crossentropy' e métrica 'accuracy'.\n",
    "# Passo 16: Use model.summary() para visualizar o modelo.\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Agora vamos de fato treinar o modelo, siga esses passos: \n",
    "##### 1. Use o callback para Ajustar a Taxa de Aprendizado \n",
    "##### ReduceLROnPlateau: Diminui a taxa de aprendizado quando uma métrica está estagnada.\n",
    "   Parâmetros:\n",
    "   - monitor='loss': Monitora a perda durante o treinamento.\n",
    "   - factor=0.4: Reduz a taxa de aprendizado por este fator.\n",
    "   - patience=2: Número de épocas sem melhora antes da redução.\n",
    "   - min_lr=0.0000001: Limite mínimo para a taxa de aprendizado.\n",
    "             \n",
    "##### 2. Treine o Modelo \n",
    "##### Utilize model.fit para iniciar o treino da rede neural.\n",
    "   Parâmetros:\n",
    "   - x_train, y_train: Conjunto de dados de treino.\n",
    "   - batch_size=64: Número de amostras por atualização de gradiente.\n",
    "   - epochs=50: Número de vezes que o modelo treina em todo o conjunto de dados.\n",
    "   - validation_data=(x_test, y_test): Conjunto de dados para validação durante o treino.\n",
    "   - callbacks=[rlrp]: Lista de callbacks a serem aplicados durante o treino.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Avaliar e Visualizar a Performance do Modelo\n",
    "\n",
    "##### Passo 1: Avaliando o Modelo\n",
    "- **Precisão nos Dados de Teste**:\n",
    "  - Use `model.evaluate(x_test, y_test)` para calcular a precisão do modelo no conjunto de teste.\n",
    "\n",
    "##### Passo 2: Preparando os Gráficos\n",
    "- **Definindo Épocas**:\n",
    "  - Crie uma lista de épocas para o eixo x\n",
    "- **Configurando o Layout do Gráfico**:\n",
    "  - Use `fig, ax = plt.subplots(1, 2)` para criar dois gráficos lado a lado.\n",
    "\n",
    "##### Passo 3: Plotando a Perda\n",
    "- **Gráfico de Perda**:\n",
    "  - Plote a perda de treino e teste:\n",
    "    ```python\n",
    "    ax[0].plot(epochs, train_loss, label='Training Loss')\n",
    "    ax[0].plot(epochs, test_loss, label='Testing Loss')\n",
    "    ```\n",
    "\n",
    "##### Passo 4: Plotando a Precisão\n",
    "- **Gráfico de Precisão**:\n",
    "  - Plote a precisão de treino e teste:\n",
    "    ```python\n",
    "    ax[1].plot(epochs, train_acc, label='Training Accuracy')\n",
    "    ax[1].plot(epochs, test_acc, label='Testing Accuracy')\n",
    "    ```\n",
    "\n",
    "##### Objetivos:\n",
    "  - **Perda**: Ajuda a identificar se o modelo está treinando bem ou se há overfitting.\n",
    "  - **Precisão**: Mostra o quão eficaz é o treinamento do modelo em acertar as previsões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Chegou a parte boa! vamos prever os valores nos dados de teste\n",
    "- Passo 1: Use model.predict() no x_test e salve o resultado em pred_test.\n",
    "- Passo 2: Crie y_pred a partir de pred_test usando inverse_transform do encoder (Precisamos converter as previsões codificadas do One-Hot de volta aos rótulos originais)\n",
    "- Passo 3: Faça o mesmo para os rótulos de teste (y_test), decodificando-os de volta aos rótulos originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rode essa célula para ver se o modelo que criamos está fazendo sentido para a maioria dos valores.\n",
    "\n",
    "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df['Predicted Labels'] = y_pred.flatten()\n",
    "df['Actual Labels'] = y_test.flatten()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Plotar Matriz de Confusão\n",
    "- Agora tenho um desafio para você, eu quero que você crie uma matriz de confusão que relacione os resultados preditos com os valores reais das emoções!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a função classification_report do sklearn para visualizar a precisão, recall e f1-score do modelo.\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusão\n",
    "- Podemos ver que nosso modelo é mais preciso na predição das emoções surpresa e raiva, o que faz sentido, pois os arquivos de áudio dessas emoções diferem bastante dos outros em aspectos como tom, velocidade, etc.\n",
    "- No geral, alcançamos 62% de precisão nos dados de teste, o que é razoável, mas podemos melhorar ainda mais aplicando mais técnicas de aumento de dados e utilizando outros métodos de extração de features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Salvando o Modelo e o Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Passo 1: Salvando o Modelo\n",
    "1. **Importação:** Use o `load_model` do Keras.\n",
    "2. **Diretório:** Crie um diretório chamado `models` se não existir.\n",
    "3. **Salvar:** Salve o modelo como no caminho especificado.\n",
    "\n",
    "##### Passo 2: Salvando o Scaler\n",
    "1. **Importação:** Use `joblib`.\n",
    "2. **Diretório:** Utilize o mesmo caminho `models`.\n",
    "3. **Salvar:** Salve o scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
